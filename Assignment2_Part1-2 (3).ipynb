{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract\n",
    "1. We are trying different imputation methods on the Titanic Dataset, and evaluating classifier accuracies for each of these. A package that we are using is fancyimpute.\n",
    "\n",
    "2. To briefly describe how gradient boosting differs from bagging.To implement gradient boosting as invoked in scikit-learn, and to evaluate classifier accuracy for the Titanic dataset.\n",
    "\n",
    "3. To theoretically, increasing the number of decision trees (n_estimators), increases classifier performance and/or generalizability. Hence to design and evaluate a computational experiment to test this, on the Titanic dataset.\n",
    "\n",
    "4. To Pick any Kaggle regression dataset. Train, tune and evaluate performance of a Random Forest Regression model and to use the feature importance calculations from this to perform feature selection and to demonstrate this using the Kaggle regression dataset that has been picked."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About the dataset \"Titanic dataset\" :\n",
    "\n",
    "The sinking of the RMS Titanic is one of the most infamous shipwrecks in history. On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. This sensational tragedy shocked the international community and led to better safety regulations for ships.\n",
    "\n",
    "One of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew. Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.\n",
    "\n",
    "In this challenge, we ask you to complete the analysis of what sorts of people were likely to survive. In particular, we ask you to apply the tools of machine learning to predict which passengers survived the tragedy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 1\n",
    "\n",
    "#### What do you mean by imputation?\n",
    "\n",
    "Imputation simply means replacing the missing values with an estimate, then analyzing the full data set as if the imputed values were actual observed values.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import necessary packages, and read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import rcParams\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "%matplotlib inline\n",
    "rcParams['figure.figsize'] = 10,8\n",
    "sns.set(style='whitegrid', palette='muted',\n",
    "        rc={'figure.figsize': (12,8)})\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "# print(os.listdir(\"../input\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data as Pandas dataframe\n",
    "train = pd.read_csv('train (2).csv')\n",
    "test = pd.read_csv('test (1).csv')\n",
    "df = pd.concat([train, test], axis=0, sort=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_all(df):\n",
    "    with pd.option_context(\"display.max_rows\", 1000, \"display.max_columns\", 1000): \n",
    "        display(df)\n",
    "\n",
    "        \n",
    "display_all(df.describe(include='all').T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Imputation\n",
    "\n",
    "We can see above that there are a few columns with missing values. The Cabin column is missing over 1000 values, so we won't use that for predictions, but the Age, Embarked and Fare columns are all complete enough that we can fill in the missing values through imputation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Impute missing age values\n",
    "\n",
    "A simple option for the missing age values is to use the median age value. Let's go a little further and use each passenger's Title to estimate their age. E.g. if a passenger has the title of Dr, I will give them the median age value for all other passengers with the same title."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract title from name\n",
    "\n",
    "We can use a regular expression to extract the title from the Name column. We will do this by finding the adjacent letters that are immediately followed by a full stop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new Title column\n",
    "df['Title'] = df['Name'].str.extract('([A-Za-z]+)\\.', expand=True)\n",
    "In [7]:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use only the most common titles\n",
    "\n",
    "Let's take a look at the unique titles across all passengers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Title'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see above, there are quite a few different titles. However, many of these titles are just French versions of the more common English titles, e.g. Mme = Madame = Mrs.\n",
    "\n",
    "We will use the six most common titles, replacing all other titles with the most appropriate of these six."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace rare titles with more common ones\n",
    "mapping = {'Mlle': 'Miss', 'Major': 'Mr', 'Col': 'Mr', 'Sir': 'Mr',\n",
    "           'Don': 'Mr', 'Mme': 'Mrs', 'Jonkheer': 'Mr', 'Lady': 'Mrs',\n",
    "           'Capt': 'Mr', 'Countess': 'Mrs', 'Ms': 'Miss', 'Dona': 'Mrs'}\n",
    "df.replace({'Title': mapping}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm that we are left with just six values\n",
    "df['Title'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use median of title group\n",
    "\n",
    "Now, for each missing age value, we will impute the age using the median age for all people with the same title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute missing Age values using median of Title groups\n",
    "title_ages = dict(df.groupby('Title')['Age'].median())\n",
    "\n",
    "# create a column of the average ages\n",
    "df['age_med'] = df['Title'].apply(lambda x: title_ages[x])\n",
    "\n",
    "# replace all missing ages with the value in this column\n",
    "df['Age'].fillna(df['age_med'], inplace=True, )\n",
    "del df['age_med']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize the median ages for each title group. Below, we see that each title has a distinctly different median age.\n",
    "\n",
    "Note: There is no risk in doing this after imputation, as the median of an age group has not been affected by our actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x='Title', y='Age', data=df, estimator=np.median, ci=None, palette='Blues_d')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='Title', data=df, palette='hls', hue='Survived')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Impute missing fare values\n",
    "For the single missing fare value, I also use the median fare value for the passenger's class.\n",
    "\n",
    "Perhaps you could come up with a cooler way of visualising the relationship between the price a passenger paid for their ticket and their chances of survival?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.swarmplot(x='Sex', y='Fare', hue='Survived', data=df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute missing Fare values using median of Pclass groups\n",
    "class_fares = dict(df.groupby('Pclass')['Fare'].median())\n",
    "\n",
    "# create a column of the average fares\n",
    "df['fare_med'] = df['Pclass'].apply(lambda x: class_fares[x])\n",
    "\n",
    "# replace all missing fares with the value in this column\n",
    "df['Fare'].fillna(df['fare_med'], inplace=True, )\n",
    "del df['fare_med']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Impute missing \"embarked\" value\n",
    "\n",
    "There are also just two missing values in the Embarked column. Here we will just use the Pandas 'backfill' method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(x='Embarked', y='Survived', data=df,\n",
    "            kind='bar', palette='muted', ci=None)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Embarked'].fillna(method='backfill', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Add family size column\n",
    "\n",
    "We can use the two variables of Parch and SibSp to create a new variable called Family_Size. This is simply done by adding Parch and SibSp together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Family_Size column (Parch +)\n",
    "df['Family_Size'] = df['Parch'] + df['SibSp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_all(df.describe(include='all').T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Save cleaned version\n",
    "\n",
    "Finally, let's save our cleaned data set so we can use it in other notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = df[pd.notnull(df['Survived'])]\n",
    "test = df[pd.isnull(df['Survived'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv('train_clean.csv', index=False)\n",
    "test.to_csv('test_clean.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion : \n",
    "\n",
    "Used the Titanic dataset and did feature engineering wherein different columns were imputed using different imputation methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contribution Statement :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did the following :\n",
    "1. Normal EDA.\n",
    "2. Imputation of data.\n",
    "3. Saving the cleaned version of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Citations:\n",
    "\n",
    "Imputation methods knowledge:---https://machinelearningmastery.com/handle-missing-data-python/\n",
    "\n",
    "Titanic dataset reference :https://www.kaggle.com/c/titanic "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## License:\n",
    "\n",
    "Copyright <2019> Ria Rajput Permission is hereby granted, free of charge, to any person obtaining a copy of this notebook and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
